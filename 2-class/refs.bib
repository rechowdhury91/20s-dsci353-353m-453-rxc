
@inproceedings{glorotUnderstandingDifficultyTraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental resul...},
  language = {en},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  month = mar,
  year = {2010},
  pages = {249-256},
  file = {D:\\zotero\\storage\\QFBVYNU7\\Glorot and Bengio - 2010 - Understanding the difficulty of training deep feed.pdf;D:\\zotero\\storage\\M93JXBJ6\\glorot10a.html}
}



% ====================== Hinton =============================

@article{maatenVisualizingDataUsing2008,
  title = {Visualizing {{Data}} Using T-{{SNE}}},
  volume = {9},
  issn = {ISSN 1533-7928},
  number = {Nov},
  journal = {Journal of Machine Learning Research},
  author = {van der Maaten, Laurens and Hinton, Geoffrey},
  year = {2008},
  pages = {2579-2605},
  file = {D:\\zotero\\storage\\3DWUC2T8\\Maaten and Hinton - 2008 - Visualizing Data using t-SNE.pdf;D:\\zotero\\storage\\PK42V8AG\\vandermaaten08a.html}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  volume = {323},
  issn = {0028-0836},
  doi = {10.1038/323533a0},
  number = {6088},
  journal = {Nature},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  month = oct,
  year = {1986},
  pages = {533-536},
  file = {D:\\zotero\\storage\\B4RIED4L\\Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  volume = {521},
  copyright = {\textcopyright{} 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {0028-0836},
  doi = {10.1038/nature14539},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.
View full text},
  language = {en},
  number = {7553},
  journal = {Nature},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  month = may,
  year = {2015},
  keywords = {Computer science,Mathematics and computing},
  pages = {436-444},
  file = {D:\\zotero\\storage\\98U77RZW\\LeCun et al. - 2015 - Deep learning.pdf;D:\\zotero\\storage\\E94N8FEH\\LeCun et al. - 2015 - Deep learning.pdf;D:\\zotero\\storage\\39RV2QPX\\nature14539.html;D:\\zotero\\storage\\EHPSCF7H\\nature14539.html}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012a,
  address = {USA},
  series = {{{NIPS}}'12},
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 1},
  publisher = {{Curran Associates Inc.}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2012},
  pages = {1097--1105},
  file = {D:\\zotero\\storage\\DXHNVSYT\\Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf;D:\\zotero\\storage\\IGHAFNHD\\4824-imagenet-classification-with-deep-convolutional-neural-networks.html;D:\\zotero\\storage\\JEERGVYP\\4824-imagenet-classification-with-deep-convolutional-neural-networks.html}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  volume = {60},
  issn = {0001-0782},
  doi = {10.1145/3065386},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  number = {6},
  journal = {Commun. ACM},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  month = may,
  year = {2017},
  pages = {84--90},
  file = {D:\\zotero\\storage\\CTGIFR3M\\Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}


% ======================== Other citations ===========================


@misc{mitchell_twitter_2013,
	title = {Twitter {News} {Consumers}: {Young}, {Mobile} and {Educated}},
	shorttitle = {Twitter {News} {Consumers}},
	url = {http://www.journalism.org/2013/11/04/twitter-news-consumers-young-mobile-and-educated/},
	abstract = {The eight percent of U.S. adults who consume news on Twitter tend to be younger, wealthier and more highly educated than Facebook users and the population overall, according to a new analysis of Twitter users.},
	urldate = {2017-10-21},
	journal = {Pew Research Center's Journalism Project},
	author = {Mitchell, Amy and Guskin, Emily},
	month = nov,
	year = {2013},
	file = {Snapshot:/media/frenchrh/OS/VUVlab/zotero/storage/FKWZJ27J/twitter-news-consumers-young-mobile-and-educated.html:text/html}
}


@book{graybill_regression_1994,
	address = {Belmont, Calif},
	edition = {1st edition},
	title = {Regression {Analysis}: {Concepts} and {Applications}},
	isbn = {978-0-534-19869-5},
	shorttitle = {Regression {Analysis}},
	abstract = {The focus of the text is on thinking clearly about and solving practical statistical problems. The approach leads from the theoretical (meaning conceptual not mathematical) to the applied, with the concept being that samples (theory) tell the investigator what needs to be known about populations (application). The authors stress regression in practice and assume that a population has a finite number of elements, which is always the case in real problems.},
	language = {English},
	publisher = {Duxbury Pr},
	author = {Graybill, Franklin A. and Iyer, Hariharan K.},
	month = jan,
	year = {1994}
}


@misc{noauthor_gas_nodate,
	title = {Gas {Mileage} of 2012 {Toyota} {Prius}},
	url = {https://www.fueleconomy.gov/feg/bymodel/2012_Toyota_Prius.shtml},
	urldate = {2017-10-21},
	file = {Gas Mileage of 2012 Toyota Prius:/media/frenchrh/OS/VUVlab/zotero/storage/F4YL998Q/2012_Toyota_Prius.html:text/html}
}


@book{wickham_ggplot2:_2016,
	address = {New York, NY},
	edition = {2nd ed. 2016 edition},
	title = {ggplot2: {Elegant} {Graphics} for {Data} {Analysis}},
	url = {https://github.com/hadley/ggplot2-book},
	isbn = {978-3-319-24275-0},
	shorttitle = {ggplot2},
	abstract = {This new edition to the classic book by ggplot2 creator Hadley Wickham highlights compatibility with knitr and RStudio. ggplot2 is a data visualization package for R that helps users create data graphics, including those that are multi-layered, with ease. With ggplot2, it's easy to:produce handsome, publication-quality plots with automatic legends created from the plot specificationsuperimpose multiple layers (points, lines, maps, tiles, box plots) from different data sources with automatically adjusted common scalesadd customizable smoothers that use powerful modeling capabilities of R, such as loess, linear models, generalized additive models, and robust regressionsave any ggplot2 plot (or part thereof) for later modification or reusecreate custom themes that capture in-house or journal style requirements and that can easily be applied to multiple plotsapproach a graph from a visual perspective, thinking about how each component of the data is represented on the final plotThis book will be useful to everyone who has struggled with displaying data in an informative and attractive way. Some basic knowledge of R is necessary (e.g., importing data into R).  ggplot2 is a mini-language specifically tailored for producing graphics, and you'll learn everything you need in the book. After reading this book you'll be able to produce graphics customized precisely for your problems, and you'll find it easy to get graphics out of your head and on to the screen or page.},
	language = {English},
	publisher = {Springer},
	author = {Wickham, Hadley},
	month = jun,
	year = {2016}
}

 
